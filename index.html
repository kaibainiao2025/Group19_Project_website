<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Multi-Machine Type Acoustic Classification on DCASE23 Task 2</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="css/style.css">
</head>
<body>
  <div class="page">

    <header>
      <h1>Multi-Machine Type Acoustic Classification on DCASE23 Task 2</h1>

      <div class="authors">
        <p>
          Dhanush Dhana Shekar<sup>1</sup> &nbsp;
          Kiara Bermudez<sup>1</sup> &nbsp;
          Wenxin Guo<sup>1</sup> &nbsp;
          Avneesh Dubey<sup>1</sup> &nbsp;
        </p>
      </div>

      <nav class="top-nav">
        <a href="#abstract">Abstract</a>
        <a href="#dataset-example">Dataset Example</a>
        <a href="#dataset-summary">Dataset Summary</a>
        <a href="#masks">Ground Truth Masks</a>
        <a href="#materials">Materials</a>
      </nav>
    </header>

    <main>

      <!-- ABSTRACT -->
      <section id="abstract">
        <h2>Abstract</h2>
        <p>
          Understanding machine sound patterns is crucial for industrial condition monitoring, 
          predictive maintenance, and safety. The DCASE 2023 Task 2 development data 
          provide 10-second mono recordings from seven machine types under normal and 
          anomalous operating conditions, together with metadata and precomputed Audio 
          Spectrogram Transformer (AST) embeddings. In this work, we construct a cleaned, 
          enriched version of this dataset by attaching audio waveforms to the metadata, 
          generating Mel spectrograms, and preserving balanced splits over machine type, 
          domain, and label. Using the AST embeddings, we first perform dimensionality 
          reduction and clustering analyses to study how well different machine types are 
          separated in the embedding space, and then train several classical classifiers 
          (Naive Bayes, Decision Tree, Random Forest, SVM, and KNN) for supervised 
          machine-type classification. We further design and train AudioResNet, a 
          ResNet-inspired convolutional neural network that operates on Mel spectrograms 
          for machine-type classification, and use its penultimate-layer features as 
          custom embeddings. These AudioResNet embeddings are evaluated with the same 
          family of classifiers and compared directly to AST embeddings in terms of 
          classification performance and representation quality. Finally, we construct
          a combined dataset with explicit anomaly indicators to support model selection 
          for anomalous vs non-anomalous classification using the learned embedding spaces.
        </p>
      </section>

      <!-- DATASET EXAMPLE -->
      <section id="dataset-example">
        <h2>Dataset Example</h2>

        <div class="figure">
          <img src="img/dataset_example.png" alt="Example minirhizotron root images from the dataset">
          <p class="caption">
            Example RGB images highlighting differences in soil type, root density, and
            appearance across species and depths.
          </p>
        </div>
      </section>

      <!-- DATASET SUMMARY -->
      <section id="dataset-summary">
        <h2>Dataset Summary</h2>

        <div class="figure">
            <img src="images/dataset_summary_machine_type.png" alt="Dataset summary visualization">
            <img src="images/dataset_summary_anomaly.png" alt="Dataset summary visualization">
        </div>
      </section>
      <!-- MASKS -->
      <section id="masks">
        <h2>Manually Annotated Ground Truth Masks</h2>

        <div class="figure">
          <img src="img/masks_example.png" alt="Example ground truth masks for root segmentation">
          <p class="caption">
            Example pairs of RGB images and corresponding binary masks used as ground truth
            for semantic segmentation.
          </p>
        </div>

        <p>
          Pixel-level annotations label each pixel as belonging to the root system or the
          surrounding soil. These masks allow quantitative evaluation of segmentation
          algorithms using metrics such as IoU, Dice score, and pixel accuracy.
        </p>
      </section>

      <!-- MATERIALS -->
      <section id="materials">
        <h2>Materials</h2>

        <div class="figure-grid">
          <figure>
            <img src="img/materials_1.png" alt="Example field or experimental setup image">
            <figcaption>Field or greenhouse setup used for data collection.</figcaption>
          </figure>

          <figure>
            <img src="img/materials_2.png" alt="Example image of sensors or tubes">
            <figcaption>Installed minirhizotron tubes and related hardware.</figcaption>
          </figure>

          <figure>
            <img src="img/materials_3.png" alt="Example image of plant material or roots">
            <figcaption>Plant material and root structures imaged in the study.</figcaption>
          </figure>
        </div>

        <p>
          Additional materials may include imaging hardware specifications, tube installation
          guidelines, and data collection protocols. These details help ensure that the
          dataset can be correctly interpreted and used as a benchmark for future work.
        </p>
      </section>
      
        <h3>Related Links</h3>
        <ul class="link-list">
          <li><a href="https://huggingface.co/datasets/renumics/dcase23-task2-enriched" target="_blank" rel="noopener">Dataset (repository)</a></li>
        </ul>
      </section>
    </main>
  </div>
</body>
</html>




